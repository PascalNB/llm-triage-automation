{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Automating the Cybersecurity Triage Process: A Comparative Study on the Performance of Large Language Models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:45:29.775343Z",
     "start_time": "2024-06-05T16:45:05.728045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "model = 'llama3:latest'\n",
    "url = 'http://localhost:11435/api/generate'\n",
    "\n",
    "\n",
    "def generate(prompt, context) -> str:\n",
    "    r = requests.post(url,\n",
    "                      json={\n",
    "                          'model': model,\n",
    "                          'prompt': prompt,\n",
    "                          'context': context,\n",
    "                      },\n",
    "                      stream=True)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    for line in r.iter_lines():\n",
    "        body = json.loads(line)\n",
    "        response_part = body.get('response', '')\n",
    "        # the response streams one token at a time, print that as we receive it\n",
    "        print(response_part, end='', flush=True)\n",
    "\n",
    "        if 'error' in body:\n",
    "            raise Exception(body['error'])\n",
    "\n",
    "        if body.get('done', False):\n",
    "            return body['context']\n",
    "\n",
    "\n",
    "context = []  # the context stores a conversation history, you can use this to make the model more context aware\n",
    "prompt = \"Hello, what are you?\"\n",
    "context = generate(prompt, context)\n",
    "print(context)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you! I'm LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm here to help answer your questions, provide information on a wide range of topics, and even generate text based on the input you provide.\n",
      "\n",
      "I'm trained on a massive dataset of text from the internet and can understand and respond to natural language inputs. This means you can chat with me just like you would with a human friend, using everyday language and syntax.\n",
      "\n",
      "So, what's on your mind? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text)![128006, 882, 128007, 271, 9906, 11, 1148, 527, 499, 30, 128009, 128006, 78191, 128007, 271, 46078, 311, 3449, 499, 0, 358, 2846, 445, 8921, 4940, 11, 459, 15592, 18328, 8040, 555, 16197, 15592, 430, 649, 3619, 323, 6013, 311, 3823, 1988, 304, 264, 7669, 1697, 11827, 13, 358, 2846, 1618, 311, 1520, 4320, 701, 4860, 11, 3493, 2038, 389, 264, 7029, 2134, 315, 13650, 11, 323, 1524, 7068, 1495, 3196, 389, 279, 1988, 499, 3493, 382, 40, 2846, 16572, 389, 264, 11191, 10550, 315, 1495, 505, 279, 7757, 323, 649, 3619, 323, 6013, 311, 5933, 4221, 11374, 13, 1115, 3445, 499, 649, 6369, 449, 757, 1120, 1093, 499, 1053, 449, 264, 3823, 4333, 11, 1701, 18254, 4221, 323, 20047, 382, 4516, 11, 1148, 596, 389, 701, 4059, 30, 3234, 499, 617, 904, 4860, 477, 13650, 499, 4265, 1093, 311, 4358, 30, 358, 2846, 682, 25212, 320, 269, 4856, 11, 682, 1495, 42395, 128009]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
