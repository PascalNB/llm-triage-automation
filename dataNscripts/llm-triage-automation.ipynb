{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Automating the Cybersecurity Triage Process: A Comparative Study on the Performance of Large Language Models"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Install required libraries, including OpenAI and Ollama"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install openai\n",
    "!pip install ollama\n",
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install matplotlib"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import libraries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T18:00:03.761686Z",
     "start_time": "2024-06-29T18:00:01.760203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import median\n",
    "from abc import abstractmethod, ABC\n",
    "from typing import Any, Callable\n",
    "from openai import AzureOpenAI\n",
    "from ollama import Client\n",
    "\n",
    "plt.style.use('tableau-colorblind10')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define Abstraction Class for Language Models and Prompts."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:34:52.246517Z",
     "start_time": "2024-06-22T13:34:52.238794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Prompt:\n",
    "\n",
    "    def __init__(self, system: str, user: str):\n",
    "        self.system = system\n",
    "        self.user = user\n",
    "\n",
    "\n",
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, model_id: str):\n",
    "        self.model_id = model_id\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: Prompt) -> dict[str, str]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class PromptGenerator:\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, input_value: any) -> Prompt:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_id(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_field(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class JsonPromptGenerator(PromptGenerator, ABC):\n",
    "\n",
    "    def __init__(self, data: dict[str, str]):\n",
    "        self.data = data\n",
    "\n",
    "    def get_field(self) -> str:\n",
    "        return self.data['field']\n",
    "\n",
    "    def get_id(self) -> str:\n",
    "        return self.data['id']\n",
    "\n",
    "\n",
    "def load_json(path: str) -> dict[str, Any]:\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Implement OpenAI Language Model.\n",
    "\n",
    "**Note:** This requires environment variables to be set."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:34:56.212463Z",
     "start_time": "2024-06-22T13:34:56.194975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OPENAI_KEY: str = os.getenv(\"OPENAI_KEY\")  # API key\n",
    "OPENAI_ENDPOINT: str = os.getenv(\"OPENAI_ENDPOINT\")  # Host URL\n",
    "OPENAI_DEPLOYMENT: str = os.getenv(\"OPENAI_DEPLOYMENT\")  # Model ID\n",
    "\n",
    "\n",
    "class OpenAILanguageModel(LanguageModel):\n",
    "    client = AzureOpenAI(azure_endpoint=OPENAI_ENDPOINT, api_key=OPENAI_KEY, api_version=\"2024-02-15-preview\")\n",
    "\n",
    "    def generate(self, prompt: Prompt) -> dict[str, str]:\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            response = OpenAILanguageModel.client.chat.completions.create(\n",
    "                model=self.model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt.system},\n",
    "                    {\"role\": \"user\", \"content\": prompt.user},\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            t1 = time.time()\n",
    "\n",
    "            return {'response': response.choices[0].message.content,\n",
    "                    'in_tokens': response.usage.prompt_tokens,\n",
    "                    'out_tokens': response.usage.completion_tokens,\n",
    "                    'time': t1 - t0}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Implement Ollama Language Model.\n",
    "\n",
    "**Note:** This requires Ollama to be running in the background on the address equal to `OLLAMA_HOST`.\n",
    "This is done by executing `ollama serve`."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:35:00.025007Z",
     "start_time": "2024-06-22T13:35:00.007966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OLLAMA_HOST = 'http://localhost:11434'\n",
    "\n",
    "\n",
    "class OllamaLanguageModel(LanguageModel):\n",
    "    client = Client(host=OLLAMA_HOST)\n",
    "\n",
    "    def generate(self, prompt: Prompt) -> dict[str, str]:\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            result = OllamaLanguageModel.client \\\n",
    "                .generate(model=self.model_id,\n",
    "                          system=prompt.system,\n",
    "                          prompt=prompt.user,\n",
    "                          format='json',\n",
    "                          stream=False)\n",
    "            # consider bug where repeated token limit is reached and output is aborted but not marked as done\n",
    "            if result['done']:\n",
    "                result['time'] = result['total_duration'] / 1e9\n",
    "            else:\n",
    "                result['time'] = time.time() - t0\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create model clients.\n",
    "\n",
    "**Note:** This script assumes that the Ollama models have already been pulled."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:35:02.636811Z",
     "start_time": "2024-06-22T13:35:02.632147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models: dict[str, LanguageModel] = {\n",
    "    'llama3': OllamaLanguageModel('llama3:8b'),  # 8b\n",
    "    'phi3': OllamaLanguageModel('phi3:14b'),  # 14b\n",
    "    'phi3-mini': OllamaLanguageModel('phi3:3.8b'),  # 3.8b\n",
    "    'aya23': OllamaLanguageModel('aya:8b'),  # 8b\n",
    "    'mistral': OllamaLanguageModel('mistral:7b'),  # 7b\n",
    "    'codellama': OllamaLanguageModel('codellama:13b'),  # 7b\n",
    "    'gemma': OllamaLanguageModel('gemma:7b'),  # 7b\n",
    "    'gemma-mini': OllamaLanguageModel('gemma:2b'),  # 2b\n",
    "    'gpt4': OpenAILanguageModel(OPENAI_DEPLOYMENT),  # 1760b\n",
    "}\n",
    "\n",
    "model_names = {k: m.model_id for k, m in models.items()}\n",
    "model_names['gpt4'] = 'GPT-4'"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setup evaluation framework"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:45:15.883541Z",
     "start_time": "2024-06-22T13:45:15.867106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_model_response(response: str, field: str) -> Any | None:\n",
    "    try:\n",
    "        return json.loads(response)[field]\n",
    "    except (TypeError, KeyError) as _:\n",
    "        return None\n",
    "\n",
    "\n",
    "def execute_all_on_model(model: LanguageModel, prompts: list[Prompt], delay: int = 0) -> list[dict[str, str]]:\n",
    "    result = []\n",
    "    first_run = True\n",
    "    for prompt in prompts:\n",
    "        if not first_run:\n",
    "            time.sleep(delay)\n",
    "        output = model.generate(prompt)  # execute prompt\n",
    "        # print(output['response'])\n",
    "        result.append(output)\n",
    "        first_run = False\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_prompts(prompt_generator: PromptGenerator, input_values: list[str]) -> list[Prompt]:\n",
    "    return [prompt_generator.generate(value) for value in input_values]\n",
    "\n",
    "\n",
    "def evaluate_model_outputs(predicted: list[bool], actual: list[bool]) -> dict[str, int]:\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for _p, _a in zip(predicted, actual):\n",
    "        if _p and _a:\n",
    "            tp += 1\n",
    "        elif _p and not _a:\n",
    "            fp += 1\n",
    "        elif not _p and _a:\n",
    "            fn += 1\n",
    "        elif not _p and not _a:\n",
    "            tn += 1\n",
    "    return {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn}\n",
    "\n",
    "\n",
    "def evaluate_model_outputs_eq(predicted: list[Any], actual: list[Any]) -> dict[str, int]:\n",
    "    t, f, = 0, 0\n",
    "    for a, b, in zip(predicted, actual):\n",
    "        print(a, b)\n",
    "        if isinstance(a, str):\n",
    "            if isinstance(b, set) or isinstance(b, list):\n",
    "                if a.lower() in b:\n",
    "                    t += 1\n",
    "                else:\n",
    "                    f += 1\n",
    "            elif a.lower() == b.lower():\n",
    "                t += 1\n",
    "            else:\n",
    "                f += 1\n",
    "        elif a == b:\n",
    "            t += 1\n",
    "        else:\n",
    "            f += 1\n",
    "    return {'t': t, 'f': f}\n",
    "\n",
    "\n",
    "def get_evaluation_statistics(tp: int, tn: int, fp: int, fn: int) -> dict[str, float]:\n",
    "    accuracy = 0.0 if sum((tp, tn, fp, fn)) == 0 else (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = 0.0 if tp == 0 else tp / (tp + fp)\n",
    "    recall = 0.0 if tp == 0 else tp / (tp + fn)\n",
    "    f1 = 0.0 if tp == 0 else 2 * precision * recall / (precision + recall)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_evaluation_statistics_eq(t: int, f: int) -> dict[str, float]:\n",
    "    accuracy = 0.0 if t == 0 else t / (t + f)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def get_model_responses(outputs: list[dict[str, str]]) -> list[str]:\n",
    "    return [output['response'] for output in outputs if 'response' in output]\n",
    "\n",
    "\n",
    "def _evaluate(model: LanguageModel,\n",
    "              prompts: list[Prompt],\n",
    "              classifications: list[Any],\n",
    "              field: str,\n",
    "              evaluation_provider: Callable[[list[Any], list[Any]], dict[str, int]],\n",
    "              statistics_provider: Callable[..., dict[str, float]],\n",
    "              delay: int = 0) -> dict[str, float]:\n",
    "    outputs: list[dict[str, str]] = execute_all_on_model(model, prompts, delay)\n",
    "    median_time = median(o['time'] for o in outputs if 'time' in o)\n",
    "    responses = get_model_responses(outputs)\n",
    "    parsed_raw = [parse_model_response(response, field) for response in responses]\n",
    "    errors = len([p for p in parsed_raw if p is None])\n",
    "    parsed = [p for p in parsed_raw if p is not None]\n",
    "    evaluation = evaluation_provider(parsed, classifications)\n",
    "    statistics = statistics_provider(**evaluation)\n",
    "    statistics['time'] = median_time\n",
    "    statistics['errors'] = errors\n",
    "    return statistics | evaluation  # join dicts\n",
    "\n",
    "\n",
    "def evaluate(model: LanguageModel,\n",
    "             prompts: list[Prompt],\n",
    "             classifications: list[bool],\n",
    "             field: str,\n",
    "             delay: int = 0) -> dict[str, float]:\n",
    "    return _evaluate(model, prompts, classifications, field,\n",
    "                     evaluate_model_outputs, get_evaluation_statistics, delay)\n",
    "\n",
    "\n",
    "def evaluate_eq(model: LanguageModel,\n",
    "                prompts: list[Prompt],\n",
    "                classifications: list[str],\n",
    "                field: str,\n",
    "                delay: int = 0) -> dict[str, float]:\n",
    "    return _evaluate(model, prompts, classifications, field,\n",
    "                     evaluate_model_outputs_eq, get_evaluation_statistics_eq, delay)\n",
    "\n",
    "\n",
    "def evaluate_all(language_models: dict[str, LanguageModel],\n",
    "                 prompt_generators: dict[str, PromptGenerator],\n",
    "                 dataset: tuple[list[str], list[Any]],\n",
    "                 evaluator: Callable[[LanguageModel, list[Prompt], list[Any], str, int], dict[str, float]] = evaluate,\n",
    "                 delay: int = 0) -> dict[str, dict[str, dict[str, float]]]:\n",
    "    prompts_dict: dict[str, list[Prompt]] = {\n",
    "        key: generate_prompts(generator, dataset[0])\n",
    "        for key, generator in prompt_generators.items()\n",
    "    }\n",
    "\n",
    "    nested: dict[str, dict[str, dict[str, float]]] = dict()\n",
    "\n",
    "    for model_id, model in language_models.items():\n",
    "        print(model_id)\n",
    "        model_result = dict()\n",
    "        nested[model_id] = model_result\n",
    "        first_run = True\n",
    "        for prompt_id, prompts in prompts_dict.items():\n",
    "            print('\\t' + prompt_id)\n",
    "            if not first_run:  # wait between prompt runs\n",
    "                time.sleep(delay)\n",
    "            evaluation = evaluator(model, prompts, dataset[1], prompt_generators[prompt_id].get_field(), delay)\n",
    "            print('\\t\\t' + str(evaluation))\n",
    "            model_result[prompt_id] = evaluation\n",
    "            first_run = False\n",
    "\n",
    "    return nested\n",
    "\n",
    "\n",
    "def transform_evaluation(nested: dict[str, dict[str, dict[str, float]]]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame.from_dict(nested, orient='index').stack().to_frame()\n",
    "    return pd.DataFrame(df[0].values.tolist(), index=df.index)\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detect Email Announcements"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:35:12.096514Z",
     "start_time": "2024-06-22T13:35:12.090478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DetectEmailPrompt(JsonPromptGenerator):\n",
    "\n",
    "    def generate(self, email: str) -> Prompt:\n",
    "        return Prompt(self.data['system'], self.data['user'] + '\\n' + email)\n",
    "\n",
    "\n",
    "detect_announcement_prompts: dict[str, PromptGenerator] = {\n",
    "    key: DetectEmailPrompt(value)\n",
    "    for key, value in load_json('data/detect_announcement_prompts.json').items()\n",
    "}"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get announcement email dataset."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "announcement_emails = pd.read_excel('data/announcement_emails.xlsx')\n",
    "announcement_emails['tactic'] = announcement_emails['tactic'].apply(lambda t: re.split(', ?', t))\n",
    "announcement_emails"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get normal emails from Enron dataset."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normal_emails = pd.read_csv('data/enron.csv', nrows=500) \\\n",
    "    .rename(columns={'Message': 'email'})[['email']]\n",
    "normal_emails_sizes = normal_emails['email'].map(len)  # get email sizes\n",
    "normal_emails = (normal_emails[(normal_emails_sizes > 100) & (normal_emails_sizes < 500)]\n",
    "                 .where(lambda x: ~x['email'].str.startswith('-' * 10))  # filter forwards\n",
    "                 .dropna()\n",
    "                 .sample(20))  # filter by email size and select 20\n",
    "normal_emails['is_announcement'] = False\n",
    "normal_emails"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Concatenate the announcement and non-announcement datasets."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emails = pd.concat([announcement_emails, normal_emails]).sample(frac=1).reset_index(drop=True)\n",
    "emails"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split into dataset for prediction and actual classification."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:04:50.766228Z",
     "start_time": "2024-06-20T18:04:50.757425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "email_bodies: list[str] = [*emails['email'].values]\n",
    "is_actual_announcement: list[bool] = [*emails['is_announcement'].values]\n",
    "detect_announcement_dataset = (email_bodies, is_actual_announcement)\n",
    "detect_announcement_dataset_size = len(emails)"
   ],
   "execution_count": 253,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run all prompts on all models."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:48:39.549234Z",
     "start_time": "2024-06-19T10:22:07.762093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "announcement_detection_evaluation_dict = evaluate_all(\n",
    "    {i: models[i] for i in models if i != 'gpt4'},\n",
    "    detect_announcement_prompts,\n",
    "    detect_announcement_dataset\n",
    ")"
   ],
   "execution_count": 134,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T11:29:16.659865Z",
     "start_time": "2024-06-19T10:51:13.211908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "announcement_detection_evaluation_dict_gpt4 = evaluate_all(\n",
    "    {'gpt4': models['gpt4']},\n",
    "    detect_announcement_prompts,\n",
    "    detect_announcement_dataset,\n",
    "    delay=7  # seconds delay between prompts to prevent timeout or token limit\n",
    ")"
   ],
   "execution_count": 138,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transform output into dataframe."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:37:00.385652Z",
     "start_time": "2024-06-20T15:37:00.360525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "announcement_detection_evaluation = transform_evaluation(\n",
    "    announcement_detection_evaluation_dict | announcement_detection_evaluation_dict_gpt4\n",
    ")\n",
    "# announcement_detection_evaluation = transform_evaluation(announcement_detection_evaluation_dict)\n",
    "announcement_detection_evaluation.index = announcement_detection_evaluation.index.rename(['Model', 'Prompt'])\n",
    "announcement_detection_evaluation = announcement_detection_evaluation.rename(index=model_names)\n",
    "announcement_detection_evaluation"
   ],
   "execution_count": 246,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Export the important metrics as a Latex table."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:05:00.511078Z",
     "start_time": "2024-06-20T18:05:00.491739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = announcement_detection_evaluation[['f1', 'time']].copy()\n",
    "df['error_rate'] = announcement_detection_evaluation['errors'] / detect_announcement_dataset_size\n",
    "df.columns.name = 'Metric'\n",
    "s = df.stack()\n",
    "s.name = 'Value'\n",
    "print(s.to_frame().reorder_levels(['Model', 'Metric', 'Prompt']).unstack(level=-1).unstack().to_latex(\n",
    "    float_format='%.3f'))"
   ],
   "execution_count": 254,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "announcement_detection_f1 = announcement_detection_evaluation['f1']\n",
    "ad_f1_axes = announcement_detection_f1.unstack() \\\n",
    "    .sort_values(by=next(iter(detect_announcement_prompts.keys())), ascending=False) \\\n",
    "    .plot(kind='barh', title='Announcement Detection Evaluation', xlabel='F1-score')\n",
    "ad_f1_axes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T17:47:03.835362Z",
     "start_time": "2024-06-19T17:47:03.643265Z"
    }
   },
   "cell_type": "code",
   "source": "ad_f1_axes.get_figure().savefig('data/announcement_detection_f1.pdf', format='pdf', bbox_inches='tight')",
   "execution_count": 165,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tactic Detection"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:35:41.238956Z",
     "start_time": "2024-06-22T13:35:41.233690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detect_tactic_prompts: dict[str, PromptGenerator] = {\n",
    "    key: DetectEmailPrompt(value)\n",
    "    for key, value in load_json('data/detect_tactic_prompts.json').items()\n",
    "}"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:35:42.729319Z",
     "start_time": "2024-06-22T13:35:42.725223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tactic_email_bodies: list[str] = [*announcement_emails['email'].values]\n",
    "tactics: list[bool] = [*announcement_emails['tactic'].values]\n",
    "detect_tactic_dataset = (tactic_email_bodies, tactics)\n",
    "detect_tactic_dataset_size = len(announcement_emails)"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T14:11:26.478953Z",
     "start_time": "2024-06-22T13:59:50.295030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tactic_detection_evaluation_dict = evaluate_all(\n",
    "    {k: m for k, m in models.items() if k != 'gpt4'},\n",
    "    detect_tactic_prompts,\n",
    "    detect_tactic_dataset,\n",
    "    evaluator=evaluate_eq\n",
    ")"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T13:59:50.293487Z",
     "start_time": "2024-06-22T13:45:26.189824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tactic_detection_evaluation_dict_gpt4 = evaluate_all(\n",
    "    {'gpt4': models['gpt4']},\n",
    "    detect_tactic_prompts,\n",
    "    detect_tactic_dataset,\n",
    "    evaluator=evaluate_eq,\n",
    "    delay=5\n",
    ")"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T14:11:57.059938Z",
     "start_time": "2024-06-22T14:11:56.899793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tactic_detection_evaluation = transform_evaluation(\n",
    "    tactic_detection_evaluation_dict | tactic_detection_evaluation_dict_gpt4\n",
    ")\n",
    "tactic_detection_evaluation.index = tactic_detection_evaluation.index.rename(['Model', 'Prompt'])\n",
    "tactic_detection_evaluation = tactic_detection_evaluation.rename(index=model_names)\n",
    "tactic_detection_evaluation"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T14:13:03.422330Z",
     "start_time": "2024-06-22T14:13:03.409849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = tactic_detection_evaluation[['accuracy', 'time']].copy()\n",
    "# df['error_rate'] = tactic_detection_evaluation['errors'] / detect_tactic_dataset_size\n",
    "df.columns.name = 'Metric'\n",
    "s = df.stack()\n",
    "s.name = 'Value'\n",
    "print(s.to_frame().reorder_levels(['Model', 'Metric', 'Prompt'])\n",
    "      .unstack(level=-1)\n",
    "      .unstack()\n",
    "      .to_latex(float_format='%.3f'))"
   ],
   "execution_count": 22,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
