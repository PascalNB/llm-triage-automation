\section{Results and Discussion}
\label{sec:discussion}

This section will give an overview of the results and highlight the key findings on the performance of LLMs in
cybersecurity tasks, as well as identify the limitations of this study.
The results show that models like GPT-4, Llama 3, Mistral and Phi-3 3.8B excelled in detecting cybersecurity
announcements, particularly with long prompts.
GPT-4 also demonstrated the highest accuracy in identifying MITRE ATT\&CK tactics.

The task of detecting cybersecurity announcements was performed using the three prompt sizes on all nine models.
An overview of the results is presented in Table\ \ref{tab:announcement-detection}.

\input{tabs/announcement-detection}

For the long prompt, GPT-4, Code Llama, Llama 3, Mistral and Phi-3\ 3.8B scored equally high with an F1-score of 1.
GPT-4 also scored high with the medium prompt with an F1-score of 0.95, which Phi-3 3.8B and Llama 3 closely followed
with scores of 0.927 and 0.9 respectively.
Llama 3 scored the highest on the task with the short prompt with a score of 0.837.

The task of detecting the MITRE ATT\&CK tactic of potential alarms following an email announcement was performed using
the two prompt sizes on all nine models.
An overview of the results is presented in Table\ \ref{tab:tactic-detection}.

\input{tabs/tactic-detection}

GPT-4 showed the highest accuracy for both the long and medium-sized prompts with scores of 0.85 and 0.8 respectively.
Mistral was generally the second-best model with accuracies of 0.7 and 0.6.
Llama 3 followed this with scores of 0.6 and 0.65.
All models performed error-free results, which is why error rates are excluded from the table.

Firstly, it is important to note that GPT-4 is the only model not operating within the same system as the other models.
Changes in median times could be attributed to increased network traffic, server requests or timeouts, but the exact
causes are unknown.
Hence, time-based metrics should generally only be compared amongst models within the same system.
However, the median times still give an indication of expected evaluation durations when incorporating GPT-4 to perform
similar tasks.
%

For all tasks of the conducted experiment, the median times of most models remained consistent regardless of prompt
size.
This suggests that prompt size does not have a significant impact on the evaluation time.
Therefore, the choice of best prompt is dependent only on the resulting performance score.

Considering the simple task of detecting announcements, only Llama 3, Mistral and Phi-3 3.8B showed relatively
consistent high scores across all three prompt sizes.
Although GPT-4 had the best scores for long and medium-sized prompts, it fell behind on the short prompt.
In general, not only did smaller models operate faster than larger models, but Llama 3 and Mistral produced overall
better results across all tasks and prompts than bigger models like Code Llama and Phi-3 14B\@.
Consequently, Llama 3, Mistral and Phi-3 3.8B are good choices when automating this simple task, as well as GPT-4 if
processing times and its proprietary nature are of no concern when integrating a model into an organization.

Notably, Phi-3 14B is the only model that struggled to adhere to the requested output format with error rates of 0.03,
0.15 and 0.75 for long, medium and short prompts respectively.
All errors can be attributed to the model's frequent inability to spell \texttt{is\_announcement} correctly.
This makes the model unreliable for simple tasks such as announcement detection.
%

On the task of detecting the MITRE ATT\&CK tactic of potentially generated alarms following a cybersecurity
announcement, only GPT-4 displayed a level of competence.
Although Llama 3 and Mistral had the second-best statistics, their accuracies can still be considered insufficient,
because an incorrect categorization for at least 30\% of the time can have drastic effects within the incident response
workflow.
All other models demonstrated a low accuracy, which highlights the difficulty in providing a comprehensive description
of the task within a single prompt.
Models like Llama 3 and Mistral should thus only be considered in cases where a short evaluation time is critical or
when an organization strives only to use openly available models.

Another noteworthy finding is that the accuracy does not significantly differ between prompt sizes.
One reason for this could be that the medium prompt is not of sufficient quality and the long prompt is of equal quality
but contains additional redundant information, resulting in equally low accuracies.
However, GPT-4's ability to accurately perform the task is a counterexample to this, which means that the other models
are simply too small or require more training or fine-tuning.
For example, smaller models might not have been trained on the required MITRE ATT\&CK date and thus lack the knowledge
to detect tactics.
Although many papers conclude that fine-tuning yields higher performances, they do not include recent LLMs such as
GPT-4 as a comparison.
Besides that, fine-tuning is out of scope for this study, which focuses solely on the performance of general LLMs.

\subsection{Limitations}
\label{subsec:limitations}

This study faces several limitations that impact the generalizability of the findings.
Firstly, the datasets of 40 emails for announcement detection and 20 emails for tactic detection are small.
These sizes limit the robustness of the results, which might not accurately reflect real-world scenarios.
To draw more definitive conclusions about the models' performances, larger and more diverse datasets are needed.

Secondly, the absence of actual alarm data hindered the ability to conduct an evaluation for other tasks that are
critical in cybersecurity triage.
This gap limits the scope of the assessment, because organizations might still desire a comparison of LLMs when
automating these tasks.

Besides that, hardware limitations restricted the study to relatively small models, because larger and more powerful
models had substantial computational requirements.
This means the study might not reflect the full potential of general LLMs.
However, the assessment of the included models is still valuable for research into smaller LLMs and environments with
limited resources.

Lastly, there is no formal separation between triage and further alarm analysis, because the only requirement in the
process is assigning a priority.
The identified steps of triage are based on examples and consultations with security analysts, but might not be
applicable in all alarm contexts or SOC environments.

Despite limitations, this study still provides a baseline for understanding how LLMs can be used to automate tasks in
cybersecurity triage, an evaluation framework to assess and compare LLMs performances, and an overall comparison of
available general LLMs.
The insights in this study pave the way to future research with other tasks, models and datasets, as well as guiding
organizations with practical implementations when optimizing triage.

\subsection{Future Works}
\label{subsec:future-works}

Avenues for further studies include using the given evaluation and comparison framework to offer more extensive
comparisons between LLMs.
Studies could include LLMs that are specifically fine-tuned for cybersecurity or other large proprietary models, as
well as the counterparts of the already-included models like Llama 3 and Mistral.

Additionally, future works can include alarm data to automate and evaluate tasks such as alarm correlation, kill
chain position identification and priority categorization.
This results in a more comprehensive assessment of LLMs' capabilities in handling real-world incidents.

Other research could also explore how triage automation affects user interaction with incident response systems, if
optimizations do decrease the strain on security analysts, and ultimately if it results in fewer security incidents and
reduced costs and damages.