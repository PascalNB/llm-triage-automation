\section{Evaluation of LLMs in Cybersecurity Triage}
\label{sec:rq2}

This section intends to answer RQ2:
\textit{What suitable evaluation metrics should be used to assess the performance of LLMs in cybersecurity triage?}
Firstly, existing evaluation metrics for LLMs are identified.
Finally, the most suitable metrics are determined to establish a testing framework for LLMs in the context of
cybersecurity triage-related tasks.

\subsection{Existing LLM Evaluation Metrics}
\label{subsec:rq2-existing-metrics}

Due to the inherent ambiguity of human language, it is challenging to evaluate the output of an LLM\@.
Outputs of LLMs are not numerical in nature, but evaluation algorithms should produce a numerical score.
This necessitates the use of sophisticated evaluation metrics.
Besides simple human evaluation techniques like expert reviews and crowdsourcing, there are some notable automated
metrics to measure LLM performance:
\begin{itemize}
    \item The BLEU\ \citep{papineni2002bleu} score is specifically designed to test machine translation by matching
    output texts with reference texts.
    \item The ROUGE\ \citep{lin2004rouge} score is used to evaluate text summaries by comparing model outputs with
    expected outputs.
\end{itemize}
These evaluation scores are purely statistical and thus reliable, but do not consider the nuances of semantics.
They demonstrate a low correlation with human judgments, particularly in tasks related to creativity and
diversity\ \citep{liu2023gpteval}.

On the other hand, NLP-based evaluation techniques are more accurate but less reliable.
Metrics such as BERTScore\ \citep{zhang2019bertscore} and BLEURT\ \citep{sellam2020bleurt} use descriptive LLMs such as
BERT to provide a score by comparing generated and reference texts while taking semantics into account.

Besides that,\ \citet{liu2023gpteval} propose G-EVAL, a framework that uses generative LLMs such as GPT-4 or
GPT-3\ \citep{brown2020language} to evaluate LLM outputs.
First, evaluation steps are generated based on a given task and evaluation criteria.
Then, the steps are used to assess an LLMs output given an input prompt and a score ranging from 1 to 5 is given.
The resulting score takes semantics into account, and the resulting evaluation is more correlated with human judgment.
However, it is unreliable due to the arbitrary nature of LLM output, and it is biased towards LLM-generated texts
compared to human-written texts.

(TODO: section about Prometheus\ \citep{kim2023prometheus})

The assigned automations in the triage process are task-specific and only require tests on the correctness of the answer
by the LLM\@.
Answers are classified as true positive, false positive, true negative and false negative, depending on the actual
classification of data the model's prediction.
From these four classifications, different evaluation metrics can be constructed.
The following task-specific metrics are suitable:
\begin{itemize}
    \item \textbf{Accuracy} is the ratio of correct predictions to the total number of answers.
    It gives an overall indication of the model's ability to make correct predictions, but can be misleading if the
    testing or real-world data is imbalanced.
    \item \textbf{Precision} is the ratio of correct answers compared to all answers that were flagged positive by the
    LLM\@.
    A high precision indicates a low false positive rate.
    \item \textbf{Recall} is the ratio of correct predictions to the total number of actual positives in the test data.
    A high recall indicates a low false negative rate.
    \item \textbf{F1-score} is a harmonic mean of precision and recall.
    As a metric, it represents a balance between precision and recall, capturing the performance using both metrics.
\end{itemize}

\subsection{Applying Evaluation Metrics to Triage}
\label{subsec:rq2-evaluating-triage}

In the triage process, a large number of false positives would cause the alarm queue to be filled up, resulting in
limited time for analysts to conduct thorough investigations.
On the other hand, a large number of false negatives would result in critical alarms being missed.
Therefore, it is important to balance these metrics when evaluating LLMs.

For the triage automations identified in section\ \ref{subsec:rq1-llms-in-context}, the metrics are applied as
follows:
\begin{itemize}
    \item (TODO: apply metrics to context) % TODO
\end{itemize}
