\section{Comparing LLMs in Cybersecurity Triage}
\label{sec:rq3}

This section intends to answer RQ3:
\textit{How do different LLMs compare in performance when optimizing the cybersecurity triage process?}
The automation steps in section\ \ref{subsec:rq1-llms-in-context} are combined with the evaluation metrics as described
in section\ \ref{subsec:rq2-evaluating-triage} to perform a comparison between various LLMs acros different tasks.
Firstly, an overview of the experiment setup is given.
After that, the comparison results are provided.

\subsection{Comparison Framework and Setup}
\label{subsec:rq3-comparison-framework}

The first LLM included in the comparison is GPT-4.
The other models selected are openly available and accessible through the Ollama\ \citep{ollama} library.
The Ollama language model platform is chosen for its ease of use and its simplicity to deploy the most recent
state-of-the-art models.
It is capable of running a local server that receives API calls, enabling the execution of operations on models from
external applications like a Jupyter Notebook.

Most models are released as a collection of multiple variants, each with a different parameter count.
A model with a large number of parameters is expected to perform better, but requires more computational resources
in contrast to a small model.
Due to computational limits, only models with parameter counts of up to 14 billion have been selected.
The selected models are Aya 23\ \citep{aryabumi2024aya}, Code Llama\ \citep{roziere2023code},
Gemma\ \citep{team2024gemma}, Llama 3, Mistral\ \citep{jiang2023mistral} and Phi-3\ \citep{abdin2024phi}.
The different models and their sizes and characteristics are given in Table\ \ref{tab:llm-selection}.
The Ollama models use a 4-bit quantization as is provided by default.

\input{tabs/llm-selection}

The evaluation framework is written in Python and contained within a Jupyter Notebook.
This supports transparency and replicability of the experiment because the code can be executed on other future LLMs to
assess and compare their performances.

For each task, prompts of different lengths are constructed to assess the impact of prompt size on the model's
performance.
Complex tasks are not possible to execute with short prompts due to the lack of sufficient information.

The data used in this study consists of 20 cybersecurity announcement emails provided by Northwave Cyber Security.
The emails are manually analyzed to determine what possible alarms could be generated as a consequence of the announced
actions, after which the MITRE ATT\&CK tactics are identified.
For example, an email announcing that a certain person will add accounts as admins to local and production servers is
assigned the following tactics: 1) privilege escalation: accounts are given higher permissions on a system;
2) persistence: maintaining high-level access by modifying permission groups ensures persistence;
3) initial access: logins through valid accounts could indicate initial access.

Additionally, 20 non-announcement emails are randomly taken from the Enron email dataset\ \citep{enron} to construct a
joined dataset of 40 labeled emails.
The code of the framework and the used data and prompts are available on
GitHub (\url{https://github.com/PascalNB/llm-triage-automation}).

Using this data, the following tasks are evaluated:
\begin{enumerate}
    \item \textbf{Announcement Detection:}
    The LLM has to determine whether a given email is a cybersecurity announcement.
    The task is executed using three separate system prompts:
    (a) A long prompt that explains the task, gives examples of security actions and requests a JSON output.
    (b) A medium-sized prompt that explains the task and briefly gives some examples of security actions.
    (c) A short prompt that briefly explains the task.

    All three prompts request the model to output its answer in JSON with a single key \texttt{is\_announcement}.
    Finally, the F1-score, median time and error rate are recorded as evaluation metrics.

    \item \textbf{Tactic Detection:}
    The LLM has to determine what the MITRE ATT\&CK tactic is of the alarms that could be generated as a result of the
    activity that is expressed in an announcement.
    These tactics are: Reconnaissance, Resource Development, Initial Access, Execution, Persistence, Privilege
    Escalation, Defense Evasion, Credential Access, Discovery, Lateral Movement, Collection, Command and Control,
    Exfiltration, and Impact.
    The task is executed using two different system prompts:
    (a) A long prompt that explains the context, gives a list of the existing tactics and requests the model to predict
    the tactic of potentially generated alarms.
    (b) A medium-sized prompt that explains the context, gives a list of the existing tactics and requests the model to
    give the tactic of the action expressed in the announcement.

    Both prompts request the model to output its answer in JSON with a single key \texttt{tactic}.
    The resulting evaluation metrics are accuracy, median time and error rate.
\end{enumerate}

Besides requesting a JSON response, both OpenAI and Ollama models can be formally instructed to exclusively output
JSON, ensuring that their outputs adhere to the correct format.
This, however, does not prevent the model from inventing its own JSON keys or values.
Such a response will be considered an error and affects the model's error rate.

Finally, all evaluations are conducted within the Jupyter Notebook running with an AMD Ryzen 7\ 4800H CPU and an NVIDIA
GeForce RTX 2060 GPU\@.