\section{Comparing LLMs in Cybersecurity Triage}
\label{sec:rq3}

This section intends to answer RQ3:
\textit{How do different LLMs compare in performance when optimizing the cybersecurity triage process?}
The automation steps in section\ \ref{subsec:rq1-llms-in-context} are combined with the evaluation metrics as described
in section\ \ref{subsec:rq2-evaluating-triage} to perform a comparison between various LLMs acros different tasks.
Firstly, an overview of the experiment setup is given.
After that, the comparison results are provided.

\subsection{Comparison Framework and Setup}
\label{subsec:rq3-comparison-framework}

The first LLM that is included in the comparison is GPT-4.
The other models selected are openly available and accessible through the Ollama\ \citep{ollama} library.
The Ollama language model platform is chosen for its ease of use and its simplicity to deploy the most recent
state-of-the-art models.
It is capable of running a local server that receives API calls, enabling the execution of operations on models from
external applications like a Jupyter Notebook.

Most models are released as a collection of multiple variants, each with a different parameter count.
A model with a large number of parameters is expected to perform better, but requires more computational resources
in contrast to a small model.
Due to computational limits, only models with parameter counts of up to 14 billion have been selected.
The selected models are Aya 23\ \citep{aryabumi2024aya}, Code Llama\ \citep{roziere2023code},
Gemma\ \citep{team2024gemma}, Llama 3, Mistral\ \citep{jiang2023mistral} and Phi-3\ \citep{abdin2024phi}.
The different models and their sizes and characteristics are as follows:
\begin{itemize}
    \item \textbf{GPT-4:} Developed by OpenAI as their fourth generation in their GPT series.
    It is estimated to have 1.76 trillion parameters.
    \item \textbf{Aya 23\ 8B:} A multilingual model developed by Cohere that supports 23 languages.
    \item \textbf{Code Llama 13B:} A fine-tuned Llama model by Meta and designed for generating and discussing code.
    \item \textbf{Gemma 7B:} Part of a family of lightweight models developed by Google DeepMind.
    \item \textbf{Gemma 2B:} The smallest model variant of the Gemma model family.
    \item \textbf{Llama 3\ 8B:} A capable pre-trained model developed by Meta and the third in their Llama series.
    \item \textbf{Mistral 7B:} A versatile model developed by Mistral AI\@.
    \item \textbf{Phi-3 14B:} A lightweight open model developed by Microsoft tailored to logic reasoning.
    \item \textbf{Phi-3 3.8B:} The smallest model variant in the Phi-3 series.
\end{itemize}

The evaluation framework is written in Python and contained within a Jupyter Notebook.
This supports transparency and replicability of the experiment because the code can be executed on other future LLMs to
assess and compare their performances.

For each task, prompts of different lengths are constructed to assess the impact of prompt size on the model's
performance.
Complex tasks are not possible to execute with short prompts due to the lack of sufficient information.

% TODO cite
The data used in this study consists of 20 cybersecurity announcement emails provided by Northwave Cyber Security.
The emails are manually analyzed to determine what possible alarms could be generated as a consequence of the announced
actions, after which the MITRE ATT\&CK tactics are identified.
Additionally, 20 non-announcement emails are randomly taken from the Enron email dataset.

Using this data, the following tasks are evaluated:
\begin{enumerate}
    \item \textbf{Announcement Detection:}
    The LLM has to determine whether a given email is a cybersecurity announcement.
    The task is executed using three separate system prompts:
    (a) A long prompt that explains the task, gives examples of security actions and requests a JSON output.
    (b) A medium-sized prompt that explains the task and briefly gives some examples of security actions.
    (c) A short prompt that briefly explains the task.

    All three prompts request the model to output its answer in JSON with a single key \texttt{is\_announcement}.
    Finally, the F1-score, median time and error rate are recorded as evaluation metrics.

    \item \textbf{Tactic Detection:}
    The LLM has to determine what the MITRE ATT\&CK tactic is of the alarms that could be generated as a result of the
    activity that is expressed in an announcement.
    These tactics are: Reconnaissance, Resource Development, Initial Access, Execution, Persistence, Privilege
    Escalation, Defense Evasion, Credential Access, Discovery, Lateral Movement, Collection, Command and Control,
    Exfiltration, and Impact.
    The task is executed using two different system prompts:
    (a) A long prompt that explains the context, gives a list of the existing tactics and requests the model to predict
    the tactic of potentially generated alarms.
    (b) A medium-sized prompt that explains the context, gives a list of the existing tactics and requests the model to
    give the tactic of the action expressed in the announcement.

    Both prompts request the model to output its answer in JSON with a single key \texttt{tactic}.
    The resulting evaluation metrics are accuracy, median time and error rate.
\end{enumerate}

Besides requesting a response in JSON, both OpenAI and Ollama models can be formally instructed to exclusively output
JSON, ensuring that their outputs adhere to the correct format.
This, however, does not prevent the model from inventing its own JSON keys or values.
Such a response will be considered an error and affects the model's error rate.

Finally, all evaluations are conducted within the Jupyter Notebook running with an AMD Ryzen 7\ 4800H CPU and an NVIDIA
GeForce RTX 2060 GPU\@.

\subsection{Comparison Results}
\label{subsec:rq3-comparison-results}

The task of detecting cybersecurity announcements was performed using the three prompt sizes on all nine models.
An overview of the results is presented in Table\ \ref{tab:announcement-detection}.

\input{tabs/announcement-detection}

For the long prompt, GPT-4, Code Llama, Llama 3, Mistral and Phi-3\ 3.8B scored equally high with an F1-score of 1.
GPT-4 also scored high with the medium prompt with an F1-score of 0.95, which Phi-3 3.8B and Llama 3 closely followed
with scores of 0.927 and 0.9 respectively.
Llama 3 scored the highest on the task with the short prompt with a score of 0.837.

The task of detecting the MITRE ATT\&CK tactic of potential alarms following an email announcement was performed using
the two prompt sizes on all nine models.
An overview of the results is presented in Table\ \ref{tab:tactic-detection}.

\input{tabs/tactic-detection}

GPT-4 showed the highest accuracy for both the long and medium-sized prompts with scores of 0.85 and 0.8 respectively.
Mistral was generally the second-best model with accuracies of 0.7 and 0.6.
Llama 3 followed this with scores of 0.6 and 0.65.
All models performed error-free results, which is why error rates are excluded from the final result.