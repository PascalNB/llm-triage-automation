\section{Comparing LLMs in Cybersecurity Triage}
\label{sec:rq3}

This section intends to answer RQ3:
\textit{How do different LLMs compare in performance when optimizing the cybersecurity triage process?}
The automation steps in section\ \ref{subsec:rq1-llms-in-context} are combined with the evaluation metrics as described
in section\ \ref{subsec:rq2-evaluating-triage} to perform a comparison between various LLMs acros different tasks.
Firstly, an overview of the experiment setup is given.
After that, the comparison results are provided.

\subsection{Comparison Framework and Setup}
\label{subsec:rq3-comparison-framework}

The first LLM that is included in the comparison is GPT-4.
The other models selected are openly available and accessible through the Ollama\ \citep{ollama} library.
The Ollama language model platform is chosen for its ease of use and its simplicity to deploy the most recent
state-of-the-art models.
It is capable of running a local server that receives API calls, enabling the execution of operations on models from
external applications like a Jupyter Notebook.

Most models are released as a collection of multiple variants, each with a different parameter count.
A model with a large number of parameters is expected to perform better, but requires more computational resources
in contrast to a small model.
Due to computational limits, only models with parameter counts of up to 14 billion have been selected.
The selected models are Aya 23\ \citep{aryabumi2024aya}, Code Llama\ \citep{roziere2023code},
Gemma\ \citep{team2024gemma}, Llama 3, Mistral\ \citep{jiang2023mistral} and Phi-3\ \citep{abdin2024phi}.
The different models and their sizes and characteristics are as follows:
\begin{itemize}
    \item \textbf{GPT-4:} Developed by OpenAI as their fourth generation in their GPT series.
    It is estimated to have 1.76 trillion parameters.
    \item \textbf{Aya 23\ 8B:} A multilingual model developed by Cohere that supports 23 languages.
    \item \textbf{Code Llama 13B:} A fine-tuned Llama model by Meta and designed for generating and discussing code.
    \item \textbf{Gemma 7B:} Part of a family of lightweight models developed by Google DeepMind.
    \item \textbf{Gemma 2B:} The smallest model variant of the Gemma model family.
    \item \textbf{Llama 3\ 8B:} A capable pre-trained model developed by Meta and the third in their Llama series.
    \item \textbf{Mistral 7B:} A versatile model developed by Mistral AI\@.
    \item \textbf{Phi-3 14B:} A lightweight open model developed by Microsoft tailored to logic reasoning.
    \item \textbf{Phi-3 3.8B:} The smallest model variant in the Phi-3 series.
\end{itemize}

The evaluation framework is written in Python and contained within a Jupyter Notebook.
This supports transparency and replicability of the experiment because the code can be executed on other future LLMs to
assess and compare their performances.

For each task, prompts of different lengths are constructed to assess the impact of prompt size on the model's
performance.
Complex tasks are not possible to execute with short prompts due to the lack of sufficient information.

% TODO cite
The data used in this study consists of 20 cybersecurity announcement emails provided by Northwave Cyber Security.
The emails are manually analyzed to determine what possible alarms could be generated as a consequence of the announced
actions, after which the MITRE ATT\&CK tactics are identified.
Additionally, 20 non-announcement emails are randomly taken from the Enron email dataset.

Using this data, the following tasks are evaluated:
\begin{enumerate}
    \item \textbf{Announcement Detection:}
    The LLM has to determine whether a given email is a cybersecurity announcement.
    The task is executed using three separate system prompts:
    (a) A long prompt that explains the task, gives examples of security actions and requests a JSON output.
    (b) A medium-sized prompt that explains the task and briefly gives some examples of security actions.
    (c) A short prompt that briefly explains the task.

    All three prompts request the model to output its answer in JSON with a single key \texttt{is\_announcement}.
    Finally, the F1-score, median time and error rate are recorded as evaluation metrics.

    \item \textbf{Tactic Detection:}
    The LLM has to determine what the MITRE ATT\&CK tactic is of the alarms that could be generated as a result of the
    activity that is expressed in an announcement.
    These tactics are: Reconnaissance, Resource Development, Initial Access, Execution, Persistence, Privilege
    Escalation, Defense Evasion, Credential Access, Discovery, Lateral Movement, Collection, Command and Control,
    Exfiltration, and Impact.
    The task is executed using two different system prompts:
    (a) A long prompt that explains the context, gives a list of the existing tactics and requests the model to predict
    the tactic of potentially generated alarms.
    (b) A medium-sized prompt that explains the context, gives a list of the existing tactics and requests the model to
    give the tactic of the action expressed in the announcement.

    Both prompts request the model to output its answer in JSON with a single key \texttt{tactic}.
    The resulting evaluation metrics are accuracy, median time and error rate.
\end{enumerate}

Besides requesting a response in JSON, both OpenAI and Ollama models can be formally instructed to exclusively output
JSON, ensuring that their outputs adhere to the correct format.
This, however, does not prevent the model from inventing its own JSON keys or values.
Such a response will be considered an error and affects the model's error rate.

\subsection{Comparison Results}
\label{subsec:rq3-comparison-results}

All evaluations were conducted within the Jupyter Notebook running with an AMD Ryzen 7\ 4800H CPU and an NVIDIA GeForce
RTX 2060 GPU\@.

It is important to note that GPT-4 is the only model not operating within the same system as the other models.
Changes in median times could be attributed to increased network traffic, server requests or timeouts, but the exact
causes will be unknown.
Hence, time-based metrics should only be compared amongst models within the same system.

\subsubsection{Announcement Detection}
The task of detection cybersecurity announcements was performed using the three prompt sizes on all nine models.
An overview of the results is presented in Table\ \ref{tab:announcement-detection}.

\input{tabs/announcement-detection}

For the long prompt, GPT-4, Code Llama, Llama 3, Mistral and Phi-3\ 3.8B score equally high with an F1-score of 1.
GPT-4 also scores high with the medium prompt with an F1-score of 0.95, which Phi-3 3.8B and Llama 3 closely follow
with scores 0.927 and 0.9 respectively.
Llama 3 scores the highest on the task with the short prompt with a score of 0.837.
Only Llama 3, Mistral and Phi-3 3.8B show relatively consistent high scores across all three prompt sizes.

Not only do smaller models operate faster in general than larger models, but Llama 3 and Mistral produce overall better
results across the three prompts than bigger models like Code Llama and Phi-3 14B\@.
Besides that, the median times of most models are consistent across all three prompt sizes, meaning that prompt size has
no significant effect on evaluation time.
Therefore, Llama 3, Mistral and Phi-3 3.8B are good choices when automating this task, as well as GPT-4 if processing
times are of no concern when integrating a model into an organization.

Notably, Phi-3 14B is the only model that struggles to adhere to the requested output format.
The error rates are 0.03, 0.15 and 0.75 for long, medium and short prompts respectively.
All errors can be attributed to the model's frequent inability to spell \texttt{is\_announcement} correctly.

\subsubsection{Announcement Tactic Detection}

The task of detecting the MITRE ATT\&CK tactic of potential alarms following an email announcement was performed using
the two prompt sizes on all nine models.
An overview of the results is presented in Table\ \ref{tab:tactic-detection}.

\input{tabs/tactic-detection}

GPT-4 shows the highest accuracy for both the long and medium-sized prompts with scores of 0.85 and 0.8 respectively.
Mistral is generally the second-best model with accuracies of 0.7 and 0.6.
Llama 3 follows this with scores of 0.6 and 0.65.
All other models demonstrate a low accuracy, which highlights the difficulty in providing a comprehensive description
of the task within a single prompt.

There is no large difference between the scores for the different prompt sizes for all models.
Similarly, the median evaluation time is unaffected by prompt size.
Moreover, all the models performed error-free results, which is why error rates are excluded in the final result.

Considering the displayed statistics, only GPT-4 is able to somewhat accurately perform the task.
Models like Llama 3 and Mistral should only be considered in cases where a short evaluation time is critical or an when
an organization strives only to use openly available models.