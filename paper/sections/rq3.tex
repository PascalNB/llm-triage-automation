\section{Comparing LLMs in Cybersecurity Triage}
\label{sec:rq3}

This section intends to answer RQ3:
\textit{How do different LLMs compare in performance when optimizing the cybersecurity triage process?}
The automation steps in section\ \ref{subsec:rq1-llms-in-context} are combined with the evaluation metrics as described
in section\ \ref{subsec:rq2-evaluating-triage} to perform a comparison between various LLMs acros different tasks.

\subsection{Comparison Framework and Setup}
\label{subsec:rq3-comparison-framework}

The first LLM that is included in the comparison is GPT-4.
The other models selected for the comparison are openly available and accessible through the
Ollama\ \citep{ollama} library.
The Ollama language model platform is chosen for its ease of use and its simplicity to deploy the most recent
state-of-the-art models.
It is capable of running a local server that receives API calls, enabling the execution of operations on models from
external applications like a Jupyter Notebook\ \citep{kluyver2016jupyter}.

Most models are released as a collection of multiple variants, each with a different parameter count.
A model with a large number of parameters is expected to perform better, but requires more computational resources
in contrast to a small model.
Due to computational limits, only models with parameter counts of up to 14 billion have been selected.
The selected models are Aya 23\ \citep{aryabumi2024aya}, Code Llama\ \citep{roziere2023code},
Gemma\ \citep{team2024gemma}, Llama 3, Mistral\ \citep{jiang2023mistral} and Phi-3\ \citep{abdin2024phi}.
The different models and their sizes and characteristics are as follows:
\begin{itemize}
    \item \textbf{GPT-4:} Developed by OpenAI as their fourth generation in their GPT series.
    \item \textbf{Aya 23\ 8B:} A multilingual model developed by Cohere that supports 23 languages.
    \item \textbf{Code Llama 13B:} A fine-tuned Llama model by Meta and designed for generating and discussing code.
    \item \textbf{Gemma 7B:} Part of a family of lightweight models developed by Google DeepMind.
    \item \textbf{Gemma 2B:} The smallest model variant of the Gemma model family.
    \item \textbf{Llama 3\ 8B:} A capable pre-trained model developed by Meta and the third in their Llama series.
    \item \textbf{Mistral 7B:} A versatile model developed by Mistral AI\@.
    \item \textbf{Phi-3 14B:} A lightweight open model developed by Microsoft tailored to logic reasoning.
    \item \textbf{Phi-3 3.8B:} The smallest model variant in the Phi-3 series.
\end{itemize}

\subsubsection{Announcement Detection}
% TODO

\subsection{Comparison Results}
\label{subsec:rq3-comparison-results}
% TODO

\subsubsection{Announcement Detection}

See table\ \ref{tab:announcement-detection}

Notably, Phi-3 Medium is the only model that struggles to adhere to the requested output format.
The error rates are 0.03, 0.15 and 0.75 for long, medium and short prompts respectively.

\input{tabs/announcement-detection}

\subsubsection{Announcement Tactic Detection}

See table\ \ref{tab:tactic-detection}

\input{tabs/tactic-detection}